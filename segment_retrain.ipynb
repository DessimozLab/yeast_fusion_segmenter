{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ac6a7c3-4e2e-4edb-90da-b8c2ee584f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 11 11 11\n",
      "{0: {'img': 'images_CNN/F10_im.TIF', 'mask': 'images_CNN/F10_mask.h5', 'gfp': 'images_CNN/F10_GFP_im.TIF', 'rfp': 'images_CNN/F10_RFP_im.TIF'}, 1: {'img': 'images_CNN/F11_im.TIF', 'mask': 'images_CNN/F11_mask.h5', 'gfp': 'images_CNN/F11_GFP_im.TIF', 'rfp': 'images_CNN/F11_RFP_im.TIF'}, 2: {'img': 'images_CNN/F1_im.TIF', 'mask': 'images_CNN/F1_mask.h5', 'gfp': 'images_CNN/F1_GFP_im.TIF', 'rfp': 'images_CNN/F1_RFP_im.TIF'}, 3: {'img': 'images_CNN/F2_im.TIF', 'mask': 'images_CNN/F2_mask.h5', 'gfp': 'images_CNN/F2_GFP_im.TIF', 'rfp': 'images_CNN/F2_RFP_im.TIF'}, 4: {'img': 'images_CNN/F3_im.TIF', 'mask': 'images_CNN/F3_mask.h5', 'gfp': 'images_CNN/F3_GFP_im.TIF', 'rfp': 'images_CNN/F3_RFP_im.TIF'}, 5: {'img': 'images_CNN/F4_im.TIF', 'mask': 'images_CNN/F4_mask.h5', 'gfp': 'images_CNN/F4_GFP_im.TIF', 'rfp': 'images_CNN/F4_RFP_im.TIF'}, 6: {'img': 'images_CNN/F5_im.TIF', 'mask': 'images_CNN/F5_mask.h5', 'gfp': 'images_CNN/F5_GFP_im.TIF', 'rfp': 'images_CNN/F5_RFP_im.TIF'}, 7: {'img': 'images_CNN/F6_im.TIF', 'mask': 'images_CNN/F6_mask.h5', 'gfp': 'images_CNN/F6_GFP_im.TIF', 'rfp': 'images_CNN/F6_RFP_im.TIF'}, 8: {'img': 'images_CNN/F7_im.TIF', 'mask': 'images_CNN/F7_mask.h5', 'gfp': 'images_CNN/F7_GFP_im.TIF', 'rfp': 'images_CNN/F7_RFP_im.TIF'}, 9: {'img': 'images_CNN/F8_im.TIF', 'mask': 'images_CNN/F8_mask.h5', 'gfp': 'images_CNN/F8_GFP_im.TIF', 'rfp': 'images_CNN/F8_RFP_im.TIF'}, 10: {'img': 'images_CNN/F9_im.TIF', 'mask': 'images_CNN/F9_mask.h5', 'gfp': 'images_CNN/F9_GFP_im.TIF', 'rfp': 'images_CNN/F9_RFP_im.TIF'}}\n"
     ]
    }
   ],
   "source": [
    "#load and vis a few seqmented images in the images_CNN folder\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageSequence\n",
    "img_list = glob.glob('images_CNN/F?_im.TIF')+glob.glob('images_CNN/F??_im.TIF')\n",
    "img_list = sorted(img_list)\n",
    "mask_list = glob.glob('images_CNN/*mask.h5')\n",
    "mask_list = sorted(mask_list)\n",
    "fluo_gfp = glob.glob('images_CNN/*GFP_im.TIF')\n",
    "fluo_gfp = sorted(fluo_gfp)\n",
    "fluo_rfp = glob.glob('images_CNN/*RFP_im.TIF')\n",
    "fluo_rfp = sorted(fluo_rfp)\n",
    "print( len(img_list), len(mask_list) , len(fluo_gfp), len(fluo_rfp  )   )\n",
    "dataset = {}\n",
    "for i in range(len(img_list)):\n",
    "    dataset[i] = { 'img': img_list[i], 'mask': mask_list[i], 'gfp': fluo_gfp[i], 'rfp': fluo_rfp[i]  }\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1ec232bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0e475907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import skimage.measure as measure\n",
    "import copy\n",
    "\n",
    "def output_contours( m , cl , verbose = False):\n",
    "    contours = measure.find_contours(m, .9)\n",
    "    if verbose:\n",
    "        plt.imshow(m)\n",
    "        for n, contour in enumerate(contours):\n",
    "            plt.plot(contour[:, 1], contour[:, 0], linewidth=2)\n",
    "        plt.show()\n",
    "\n",
    "    #output contours of each mask to file\n",
    "    #divide x and y coordinates by total image size\n",
    "    #to get values between 0 and 1  \n",
    "    #<class-index> <x1> <y1> <x2> <y2> ... <xn> <yn>\n",
    "    lines = []\n",
    "    for c in contours:\n",
    "        coords = []\n",
    "        for i in range(0,c.shape[0],4):\n",
    "            coords.append( float(c[i][0]) / m.shape[0] )\n",
    "            coords.append( float(c[i][1]) / m.shape[1])\n",
    "        line = str(cl) + ' ' + ' '.join([str(c) for c in coords]) + '\\n'\n",
    "        if verbose == True:\n",
    "            print(line)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def split_mask(mask, crop = 1024):\n",
    "    #custom encoding with 3 classes\n",
    "    mask = mask[0:crop, 0:crop]\n",
    "    mask1 = copy.deepcopy(mask)\n",
    "    mask1[ (mask1 > 0) & (mask1 < 1000)] =  1\n",
    "    mask1[mask1 > 1] = 0 \n",
    "\n",
    "    mask2 = copy.deepcopy(mask)\n",
    "    mask2[mask2 == 1] = 0\n",
    "    mask2[ (mask2 > 1000) & (mask2 < 2000)] =  1\n",
    "    mask2[mask2 > 1] = 0\n",
    "\n",
    "    mask3 = copy.deepcopy(mask)\n",
    "    mask3[mask3 == 1] = 0\n",
    "    mask3[ (mask3 >= 2000) ] =  1\n",
    "    mask3[mask3 > 1] = 0\n",
    "    return mask1, mask2, mask3\n",
    "\n",
    "def mask2contourfile( mask , outputfile , verbose = False):\n",
    "    m1, m2, m3 = split_mask(mask)\n",
    "    lines = output_contours(m1, 1 , verbose = verbose)\n",
    "    lines += output_contours(m2, 2, verbose = verbose)\n",
    "    lines += output_contours(m3, 3, verbose = verbose)\n",
    "    \n",
    "    with open(outputfile, 'w') as f:\n",
    "        for l in lines:\n",
    "            f.write(l)\n",
    "    return  outputfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f791e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean finaldataset folder\n",
    "import shutil\n",
    "overwite = True\n",
    "if overwite:\n",
    "    try:\n",
    "        shutil.rmtree('./datasets/')\n",
    "    except:\n",
    "        pass\n",
    "    os.mkdir('./datasets/')\n",
    "    os.mkdir('./datasets/train')\n",
    "    \n",
    "    os.mkdir('./datasets/train/images/')\n",
    "    os.mkdir('./datasets/train/labels/')\n",
    "    os.mkdir('./datasets/val/')\n",
    "    os.mkdir('./datasets/val/images')\n",
    "    os.mkdir('./datasets/val/labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6c616bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1020 1021 1023 1024 1025 1026 1027 1028 1029 2001 2002 2003]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 2001 2002]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7    8    9   10   11   12 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 2001 2002 2003\n",
      " 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7    8    9   10   13   14   15   16   17   18 1001 1002 1003 1004 1007 1008 1009 1010 1013 1014 1015 1016 1017 1018 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1046 1047 1048 1049 1050 1051 1052\n",
      " 1053 1054 1055 1056 1059 1060 1061 1062 1063 1064 1065 1066 1067 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 2001 2002 2003 2004 2005 2006]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\n",
      " 2016 2017 2018 2019 2020]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24 1001 1003 1004 1005 1006 1007 1009 1010 1011 1012 1013 1014 1015 1017 1018 1019 1020 1021 1022 1023 1026 1027 1028 1029 1030 1031 1032 1033 1034 1036 1037 1038 1039 1040 1042 1043 1044 1045\n",
      " 1046 1047 1051 1052 1053 1055 1056 1057 1058 1059 1061 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7 1001 1002 1003 1004 1005 1006 1007 1008 1012 1013 1015 1016 1017 1018 1019 1020 1021 1022 1024 1025 1026 1027 1028 1029 1031 1032 1033 1034 1035 2001 2002 2003 2004 2005 2006]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4 1001 1002 1003 1004 1005 1007 1008 1009 1010 2001]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5 1001 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1032 1033 1034 1035 1036 1037 2001 2002 2003]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4 1001 1002 1003 1004 1005 1006 1007 1008 1009 1011 1012 1013 1014 1015 1016 1018 1019 1021 1022 1023 1024 1025 1026 2001 2002 2003]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.38it/s]\n"
     ]
    }
   ],
   "source": [
    "#stack equivalent frames together from img, gfp and rfp and transform them to a jpg image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "verbose = False\n",
    "\n",
    "def yield_frames(img,crop=1024):\n",
    "    for i, page in enumerate(ImageSequence.Iterator(img)):\n",
    "        page = np.array(page)[0:crop, 0:crop]\n",
    "        yield page\n",
    "count = 0\n",
    "\n",
    "for sample in dataset:\n",
    "    maskfile = dataset[sample]['mask']\n",
    "    maskh5 = h5py.File(maskfile, 'r')\n",
    "    for group in maskh5.keys():\n",
    "        for frame in maskh5[group]:\n",
    "            mask = np.array( maskh5[group][frame] ,  dtype = np.uint16 )\n",
    "            if np.sum(mask) > 0 :\n",
    "                print( group, frame)\n",
    "                print(np.unique(mask) )\n",
    "                if verbose == True:\n",
    "                    plt.imshow(mask)\n",
    "                    plt.show()\n",
    "                converted = mask2contourfile(mask, maskfile +'converted.txt' , verbose = verbose)\n",
    "                dataset[sample]['mask_poly'] = converted\n",
    "                break\n",
    "\n",
    "    print('loading img')\n",
    "    img = Image.open(dataset[sample]['img'])\n",
    "    img = [frame for frame in yield_frames(img)]\n",
    "    print('loading gfp')\n",
    "\n",
    "    gfp = Image.open(dataset[sample]['gfp'])\n",
    "    gfp = [frame for frame in yield_frames(gfp)]\n",
    "    \n",
    "    print('loading rfp')\n",
    "    rfp = Image.open(dataset[sample]['rfp'])\n",
    "    rfp = [frame for frame in yield_frames(rfp)]\n",
    "\n",
    "    #stack the frames together\n",
    "    for i in tqdm.tqdm(range(len(img))):\n",
    "        im = np.stack([img[i], gfp[i], rfp[i]], axis=-1)\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite('./datasets/train/images/img_'+str(count)+'.png', im )\n",
    "        #save the correct mask file\n",
    "        shutil.copyfile(dataset[sample]['mask_poly'], './datasets/train/labels/img_'+str(count)+'.txt' )\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0109e6e1-c085-47de-80a8-837beab7848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move a fraction of the training data and corresponding labels to val\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "files = os.listdir('./datasets/train/images/')\n",
    "random.shuffle(files)\n",
    "val_files = files[:int(len(files)*.1)]\n",
    "for f in val_files:\n",
    "    shutil.move('./datasets/train/images/'+f, './datasets/val/images/'+f)\n",
    "    shutil.move('./datasets/train/labels/'+f.replace('.png', '.txt'), './datasets/val/labels/'+f.replace('.png', '.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2f4d2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augment the images and masks\n",
    "#move files into train and test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "83866e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create traininging yaml file for the dataset\n",
    "\n",
    "outyaml = \"\"\"# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "path: ./  # dataset root dir\n",
    "train: train  \n",
    "val: val  \n",
    "test:  # test images (optional)\n",
    "\n",
    "names: \n",
    "    0:'fused',\n",
    "    1:'halted',\n",
    "    2:'lysed'\n",
    "\"\"\"\n",
    "\n",
    "with open('./dataset.yaml', 'w') as f:\n",
    "    f.write(outyaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ebdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.9 ðŸš€ Python-3.10.8 torch-1.13.1 CPU (AMD EPYC 7443 24-Core Processor)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=./dataset.yaml, epochs=3, time=None, patience=50, batch=8, imgsz=1024, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train38, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train38\n",
      "Overriding model.yaml nc=80 with nc=32\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1010320  ultralytics.nn.modules.head.Segment          [32, 32, 64, [64, 128, 256]]  \n",
      "YOLOv8n-seg summary: 261 layers, 3269856 parameters, 3269840 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train38', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /work/FAC/FBM/DBC/cdessim2/default/dmoi/projects/segmentation_fluo/datasets/train/labels.cache... 70 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /work/FAC/FBM/DBC/cdessim2/default/dmoi/projects/segmentation_fluo/datasets/val/labels.cache... 7 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/segment/train38/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000278, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 1024 train, 1024 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train38\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3         0G       3.77      6.037      5.629      2.888        329       1024:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [01:00<01:16, 15.21s/it]"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "results = model.train(\n",
    "        batch=8,\n",
    "        device=\"cpu\",\n",
    "        data='./dataset.yaml',\n",
    "        epochs=3,\n",
    "        imgsz=1024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as show_image\n",
    "show_image(filename=\"runs/segment/train60/val_batch0_labels.jpg\")\n",
    "\n",
    "show_image(filename=\"runs/segment/train60/MaskP_curve.png\")\n",
    "\n",
    "show_image(filename=\"runs/segment/train60/results.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcbc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames\tframee\tcuttop\tcutbttm\tcutleft\tcutrght\n",
      "0\t6\t1\t1040\t1\t1392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#image augmentation for training\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def random_rotation(image, mask, angle_range):\n",
    "    angle = random.uniform(-angle_range, angle_range)\n",
    "    image = Image.fromarray(image)\n",
    "    mask = Image.fromarray(mask)\n",
    "    image = image.rotate(angle)\n",
    "    mask = mask.rotate(angle)\n",
    "    return np.array(image), np.array(mask)\n",
    "\n",
    "def random_flip(image, mask):\n",
    "    if random.random() > 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "    return image, mask\n",
    "\n",
    "def random_crop(image, mask, crop_size):\n",
    "    h, w, _ = image.shape\n",
    "    crop_h, crop_w = crop_size\n",
    "    top = np.random.randint(0, h - crop_h)\n",
    "    left = np.random.randint(0, w - crop_w)\n",
    "    bottom = top + crop_h\n",
    "    right = left + crop_w\n",
    "    image = image[top:bottom, left:right]\n",
    "    mask = mask[top:bottom, left:right]\n",
    "    return image, mask\n",
    "\n",
    "def random_augmentation(image, mask, angle_range, crop_size):\n",
    "    image, mask = random_rotation(image, mask, angle_range)\n",
    "    image, mask = random_flip(image, mask)\n",
    "    image, mask = random_crop(image, mask, crop_size)\n",
    "    return image, mask\n",
    "\n",
    "#resize to original size\n",
    "def resize(image, mask, size):\n",
    "    image = cv2.resize(image, size)\n",
    "    mask = cv2.resize(mask, size)\n",
    "    return image, mask\n",
    "\n",
    "#apply augmentation and then resize to original size\n",
    "def augment_and_resize(image, mask, angle_range, crop_size, size):\n",
    "    image, mask = random_augmentation(image, mask, angle_range, crop_size)\n",
    "    image, mask = resize(image, mask, size)\n",
    "    return image, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc174d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#augment dataset and dump to disk in pt format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e81b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain yolo segmentation net on the new data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.dataset[idx]['img'])\n",
    "        maskfile = h5py.File(self.dataset[idx]['mask'], 'r')\n",
    "        \n",
    "        for group in maskfile:\n",
    "            for frame in maskfile[group]:\n",
    "                mask = np.array( maskfile[group][frame] ,  dtype = np.uint16 )\n",
    "                if np.sum(mask) > 0 :\n",
    "                    mask = mask\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "transform = transforms.Compose([ transforms.ToTensor() ])\n",
    "dataset = CustomDataset(dataset, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traininig loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        print(outputs.shape, targets.shape)\n",
    "        break\n",
    "\n",
    "\n",
    "#visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "for i in range(2):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.array(inputs[i].permute(1, 2, 0)))\n",
    "    plt.title('Input')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(targets[i].squeeze(), cmap='gray')\n",
    "    plt.title('Target')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(outputs[i].squeeze(), cmap='gray')\n",
    "    plt.title('Output')\n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
