{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0ac6a7c3-4e2e-4edb-90da-b8c2ee584f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 11 11 11\n",
      "{0: {'img': 'images_CNN/F10_im.TIF', 'mask': 'images_CNN/F10_mask.h5', 'gfp': 'images_CNN/F10_GFP_im.TIF', 'rfp': 'images_CNN/F10_RFP_im.TIF'}, 1: {'img': 'images_CNN/F11_im.TIF', 'mask': 'images_CNN/F11_mask.h5', 'gfp': 'images_CNN/F11_GFP_im.TIF', 'rfp': 'images_CNN/F11_RFP_im.TIF'}, 2: {'img': 'images_CNN/F1_im.TIF', 'mask': 'images_CNN/F1_mask.h5', 'gfp': 'images_CNN/F1_GFP_im.TIF', 'rfp': 'images_CNN/F1_RFP_im.TIF'}, 3: {'img': 'images_CNN/F2_im.TIF', 'mask': 'images_CNN/F2_mask.h5', 'gfp': 'images_CNN/F2_GFP_im.TIF', 'rfp': 'images_CNN/F2_RFP_im.TIF'}, 4: {'img': 'images_CNN/F3_im.TIF', 'mask': 'images_CNN/F3_mask.h5', 'gfp': 'images_CNN/F3_GFP_im.TIF', 'rfp': 'images_CNN/F3_RFP_im.TIF'}, 5: {'img': 'images_CNN/F4_im.TIF', 'mask': 'images_CNN/F4_mask.h5', 'gfp': 'images_CNN/F4_GFP_im.TIF', 'rfp': 'images_CNN/F4_RFP_im.TIF'}, 6: {'img': 'images_CNN/F5_im.TIF', 'mask': 'images_CNN/F5_mask.h5', 'gfp': 'images_CNN/F5_GFP_im.TIF', 'rfp': 'images_CNN/F5_RFP_im.TIF'}, 7: {'img': 'images_CNN/F6_im.TIF', 'mask': 'images_CNN/F6_mask.h5', 'gfp': 'images_CNN/F6_GFP_im.TIF', 'rfp': 'images_CNN/F6_RFP_im.TIF'}, 8: {'img': 'images_CNN/F7_im.TIF', 'mask': 'images_CNN/F7_mask.h5', 'gfp': 'images_CNN/F7_GFP_im.TIF', 'rfp': 'images_CNN/F7_RFP_im.TIF'}, 9: {'img': 'images_CNN/F8_im.TIF', 'mask': 'images_CNN/F8_mask.h5', 'gfp': 'images_CNN/F8_GFP_im.TIF', 'rfp': 'images_CNN/F8_RFP_im.TIF'}, 10: {'img': 'images_CNN/F9_im.TIF', 'mask': 'images_CNN/F9_mask.h5', 'gfp': 'images_CNN/F9_GFP_im.TIF', 'rfp': 'images_CNN/F9_RFP_im.TIF'}}\n"
     ]
    }
   ],
   "source": [
    "#load and vis a few seqmented images in the images_CNN folder\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageSequence\n",
    "img_list = glob.glob('images_CNN/F?_im.TIF')+glob.glob('images_CNN/F??_im.TIF')\n",
    "img_list = sorted(img_list)\n",
    "mask_list = glob.glob('images_CNN/*mask.h5')\n",
    "mask_list = sorted(mask_list)\n",
    "fluo_gfp = glob.glob('images_CNN/*GFP_im.TIF')\n",
    "fluo_gfp = sorted(fluo_gfp)\n",
    "fluo_rfp = glob.glob('images_CNN/*RFP_im.TIF')\n",
    "fluo_rfp = sorted(fluo_rfp)\n",
    "print( len(img_list), len(mask_list) , len(fluo_gfp), len(fluo_rfp  )   )\n",
    "dataset = {}\n",
    "for i in range(len(img_list)):\n",
    "    dataset[i] = { 'img': img_list[i], 'mask': mask_list[i], 'gfp': fluo_gfp[i], 'rfp': fluo_rfp[i]  }\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0e475907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import skimage.measure as measure\n",
    "import copy\n",
    "\n",
    "def output_contours( m , cl , verbose = False):\n",
    "    contours = measure.find_contours(m, .9)\n",
    "    if verbose:\n",
    "        plt.imshow(m)\n",
    "        for n, contour in enumerate(contours):\n",
    "            plt.plot(contour[:, 1], contour[:, 0], linewidth=2)\n",
    "        plt.show()\n",
    "\n",
    "    #output contours of each mask to file\n",
    "    #divide x and y coordinates by total image size\n",
    "    #to get values between 0 and 1  \n",
    "    #<class-index> <x1> <y1> <x2> <y2> ... <xn> <yn>\n",
    "    lines = []\n",
    "    for c in contours:\n",
    "        coords = []\n",
    "        for i in range(0,c.shape[0],4):\n",
    "            coords.append( float(c[i][0]) / m.shape[0] )\n",
    "            coords.append( float(c[i][1]) / m.shape[1])\n",
    "        line = str(cl) + ' ' + ' '.join([str(c) for c in coords]) + '\\n'\n",
    "        if verbose == True:\n",
    "            print(line)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def split_mask(mask, crop = 1024):\n",
    "    #custom encoding with 3 classes\n",
    "    mask = mask[0:crop, 0:crop]\n",
    "    mask1 = copy.deepcopy(mask)\n",
    "    mask1[ (mask1 > 0) & (mask1 < 1000)] =  1\n",
    "    mask1[mask1 > 1] = 0 \n",
    "\n",
    "    mask2 = copy.deepcopy(mask)\n",
    "    mask2[mask2 == 1] = 0\n",
    "    mask2[ (mask2 > 1000) & (mask2 < 2000)] =  1\n",
    "    mask2[mask2 > 1] = 0\n",
    "\n",
    "    mask3 = copy.deepcopy(mask)\n",
    "    mask3[mask3 == 1] = 0\n",
    "    mask3[ (mask3 >= 2000) ] =  1\n",
    "    mask3[mask3 > 1] = 0\n",
    "    return mask1, mask2, mask3\n",
    "\n",
    "def mask2contourfile( mask , outputfile , verbose = False):\n",
    "    m1, m2, m3 = split_mask(mask)\n",
    "    lines = output_contours(m1, 1 , verbose = verbose)\n",
    "    lines += output_contours(m2, 2, verbose = verbose)\n",
    "    lines += output_contours(m3, 3, verbose = verbose)\n",
    "    \n",
    "    with open(outputfile, 'w') as f:\n",
    "        for l in lines:\n",
    "            f.write(l)\n",
    "    return  outputfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f791e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean finaldataset folder\n",
    "import shutil\n",
    "overwite = True\n",
    "if overwite:\n",
    "    try:\n",
    "        shutil.rmtree('./datasets/')\n",
    "    except:\n",
    "        pass\n",
    "    os.mkdir('./datasets/')\n",
    "    os.mkdir('./datasets/train')\n",
    "    \n",
    "    os.mkdir('./datasets/train/images/')\n",
    "    os.mkdir('./datasets/train/labels/')\n",
    "    os.mkdir('./datasets/val/')\n",
    "    os.mkdir('./datasets/val/images')\n",
    "    os.mkdir('./datasets/val/labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6c616bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1020 1021 1023 1024 1025 1026 1027 1028 1029 2001 2002 2003]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 2001 2002]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7    8    9   10   11   12 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 2001 2002 2003\n",
      " 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7    8    9   10   13   14   15   16   17   18 1001 1002 1003 1004 1007 1008 1009 1010 1013 1014 1015 1016 1017 1018 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1046 1047 1048 1049 1050 1051 1052\n",
      " 1053 1054 1055 1056 1059 1060 1061 1062 1063 1064 1065 1066 1067 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 2001 2002 2003 2004 2005 2006]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\n",
      " 2016 2017 2018 2019 2020]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24 1001 1003 1004 1005 1006 1007 1009 1010 1011 1012 1013 1014 1015 1017 1018 1019 1020 1021 1022 1023 1026 1027 1028 1029 1030 1031 1032 1033 1034 1036 1037 1038 1039 1040 1042 1043 1044 1045\n",
      " 1046 1047 1051 1052 1053 1055 1056 1057 1058 1059 1061 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5    6    7 1001 1002 1003 1004 1005 1006 1007 1008 1012 1013 1015 1016 1017 1018 1019 1020 1021 1022 1024 1025 1026 1027 1028 1029 1031 1032 1033 1034 1035 2001 2002 2003 2004 2005 2006]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4 1001 1002 1003 1004 1005 1007 1008 1009 1010 2001]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4    5 1001 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1032 1033 1034 1035 1036 1037 2001 2002 2003]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0 T5\n",
      "[   0    1    2    3    4 1001 1002 1003 1004 1005 1006 1007 1008 1009 1011 1012 1013 1014 1015 1016 1018 1019 1021 1022 1023 1024 1025 1026 2001 2002 2003]\n",
      "loading img\n",
      "loading gfp\n",
      "loading rfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 13.22it/s]\n"
     ]
    }
   ],
   "source": [
    "#stack equivalent frames together from img, gfp and rfp and transform them to a jpg image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "verbose = False\n",
    "\n",
    "def yield_frames(img,crop=1024):\n",
    "    for i, page in enumerate(ImageSequence.Iterator(img)):\n",
    "        page = np.array(page)[0:crop, 0:crop]\n",
    "        yield page\n",
    "count = 0\n",
    "\n",
    "for sample in dataset:\n",
    "    maskfile = dataset[sample]['mask']\n",
    "    maskh5 = h5py.File(maskfile, 'r')\n",
    "    for group in maskh5.keys():\n",
    "        for frame in maskh5[group]:\n",
    "            mask = np.array( maskh5[group][frame] ,  dtype = np.uint16 )\n",
    "            if np.sum(mask) > 0 :\n",
    "                print( group, frame)\n",
    "                print(np.unique(mask) )\n",
    "                if verbose == True:\n",
    "                    plt.imshow(mask)\n",
    "                    plt.show()\n",
    "                converted = mask2contourfile(mask, maskfile +'converted.txt' , verbose = verbose)\n",
    "                dataset[sample]['mask_poly'] = converted\n",
    "                break\n",
    "\n",
    "    print('loading img')\n",
    "    img = Image.open(dataset[sample]['img'])\n",
    "    img = [frame for frame in yield_frames(img)]\n",
    "    print('loading gfp')\n",
    "\n",
    "    gfp = Image.open(dataset[sample]['gfp'])\n",
    "    gfp = [frame for frame in yield_frames(gfp)]\n",
    "    \n",
    "    print('loading rfp')\n",
    "    rfp = Image.open(dataset[sample]['rfp'])\n",
    "    rfp = [frame for frame in yield_frames(rfp)]\n",
    "\n",
    "    #stack the frames together\n",
    "    for i in tqdm.tqdm(range(len(img))):\n",
    "        im = np.stack([img[i], gfp[i], rfp[i]], axis=-1)\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite('./datasets/train/images/img_'+str(count)+'.png', im )\n",
    "        #save the correct mask file\n",
    "        shutil.copyfile(dataset[sample]['mask_poly'], './datasets/train/labels/img_'+str(count)+'.txt' )\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0109e6e1-c085-47de-80a8-837beab7848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move a fraction of the training data and corresponding labels to val\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "files = os.listdir('./datasets/train/images/')\n",
    "random.shuffle(files)\n",
    "val_files = files[:int(len(files)*.1)]\n",
    "for f in val_files:\n",
    "    shutil.move('./datasets/train/images/'+f, './datasets/val/images/'+f)\n",
    "    shutil.move('./datasets/train/labels/'+f.replace('.png', '.txt'), './datasets/val/labels/'+f.replace('.png', '.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2f4d2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augment the images and masks\n",
    "#move files into train and test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "83866e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create traininging yaml file for the dataset\n",
    "\n",
    "outyaml = \"\"\"# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "path: ./  # dataset root dir\n",
    "train: train  # train images (relative to 'path') 4 images\n",
    "val: val  # val/images images (relative to 'path') 4 images\n",
    "test:  # test images (optional)\n",
    "\n",
    "names: ['fused','halted', 'lysed']\n",
    "\"\"\"\n",
    "\n",
    "with open('./datasets/dataset.yaml', 'w') as f:\n",
    "    f.write(outyaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "062ebdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.9 üöÄ Python-3.10.8 torch-1.13.1 CPU (AMD EPYC 7443 24-Core Processor)\n",
      "WARNING ‚ö†Ô∏è Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=./datasets/dataset.yaml, epochs=7, time=None, patience=50, batch=8, imgsz=1000, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train26, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train26\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004665  ultralytics.nn.modules.head.Segment          [3, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3264201 parameters, 3264185 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train26', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING ‚ö†Ô∏è imgsz=[1000] must be multiple of max stride 32, updating to [1024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /work/FAC/FBM/DBC/cdessim2/default/dmoi/projects/segmentation_fluo/datasets/train/labels... 70 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:00<00:00, 237.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /work/FAC/FBM/DBC/cdessim2/default/dmoi/projects/segmentation_fluo/datasets/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /work/FAC/FBM/DBC/cdessim2/default/dmoi/projects/segmentation_fluo/datasets/val/labels... 7 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 165.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /work/FAC/FBM/DBC/cdessim2/default/dmoi/projects/segmentation_fluo/datasets/val/labels.cache\n",
      "Plotting labels to runs/segment/train26/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 1024 train, 1024 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train26\u001b[0m\n",
      "Starting training for 7 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n-seg.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets/dataset.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/ultralytics/engine/model.py:601\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/ultralytics/engine/trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/ultralytics/engine/trainer.py:376\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    375\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/ultralytics/nn/tasks.py:79\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/ultralytics/nn/tasks.py:258\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    257\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/ultralytics/utils/loss.py:295\u001b[0m, in \u001b[0;36mv8SegmentationLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Pboxes\u001b[39;00m\n\u001b[1;32m    293\u001b[0m pred_bboxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_decode(anchor_points, pred_distri)  \u001b[38;5;66;03m# xyxy, (b, h*w, 4)\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m _, target_bboxes, target_scores, fg_mask, target_gt_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massigner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m target_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(target_scores\u001b[38;5;241m.\u001b[39msum(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Cls loss\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\u001b[39;00m\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/ultralytics/utils/tal.py:72\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.forward\u001b[0;34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m     63\u001b[0m     device \u001b[38;5;241m=\u001b[39m gt_bboxes\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     65\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfull_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbg_idx)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     66\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_bboxes)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m mask_pos, align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pos_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m target_gt_idx, fg_mask, mask_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_highest_overlaps(mask_pos, overlaps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_max_boxes)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Assigned target\u001b[39;00m\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/ultralytics/utils/tal.py:94\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.get_pos_mask\u001b[0;34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)\u001b[0m\n\u001b[1;32m     92\u001b[0m mask_in_gts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_candidates_in_gts(anc_points, gt_bboxes)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Get anchor_align metric, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_box_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_in_gts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Get topk_metric mask, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m mask_topk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_topk_candidates(align_metric, topk_mask\u001b[38;5;241m=\u001b[39mmask_gt\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopk)\u001b[38;5;241m.\u001b[39mbool())\n",
      "File \u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages/ultralytics/utils/tal.py:113\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.get_box_metrics\u001b[0;34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m    111\u001b[0m ind[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m gt_labels\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# b, max_num_obj\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Get the scores of each grid for each gt cls\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m bbox_scores[mask_gt] \u001b[38;5;241m=\u001b[39m \u001b[43mpd_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m[mask_gt]  \u001b[38;5;66;03m# b, max_num_obj, h*w\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# (b, max_num_obj, 1, 4), (b, 1, h*w, 4)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m pd_boxes \u001b[38;5;241m=\u001b[39m pd_bboxes\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_max_boxes, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[mask_gt]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 1 with size 3"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "results = model.train(\n",
    "        batch=8,\n",
    "        device=\"cpu\",\n",
    "        data='./datasets/dataset.yaml',\n",
    "        epochs=7,\n",
    "        imgsz=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as show_image\n",
    "show_image(filename=\"runs/segment/train60/val_batch0_labels.jpg\")\n",
    "\n",
    "show_image(filename=\"runs/segment/train60/MaskP_curve.png\")\n",
    "\n",
    "show_image(filename=\"runs/segment/train60/results.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcbc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames\tframee\tcuttop\tcutbttm\tcutleft\tcutrght\n",
      "0\t6\t1\t1040\t1\t1392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#image augmentation for training\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def random_rotation(image, mask, angle_range):\n",
    "    angle = random.uniform(-angle_range, angle_range)\n",
    "    image = Image.fromarray(image)\n",
    "    mask = Image.fromarray(mask)\n",
    "    image = image.rotate(angle)\n",
    "    mask = mask.rotate(angle)\n",
    "    return np.array(image), np.array(mask)\n",
    "\n",
    "def random_flip(image, mask):\n",
    "    if random.random() > 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "    return image, mask\n",
    "\n",
    "def random_crop(image, mask, crop_size):\n",
    "    h, w, _ = image.shape\n",
    "    crop_h, crop_w = crop_size\n",
    "    top = np.random.randint(0, h - crop_h)\n",
    "    left = np.random.randint(0, w - crop_w)\n",
    "    bottom = top + crop_h\n",
    "    right = left + crop_w\n",
    "    image = image[top:bottom, left:right]\n",
    "    mask = mask[top:bottom, left:right]\n",
    "    return image, mask\n",
    "\n",
    "def random_augmentation(image, mask, angle_range, crop_size):\n",
    "    image, mask = random_rotation(image, mask, angle_range)\n",
    "    image, mask = random_flip(image, mask)\n",
    "    image, mask = random_crop(image, mask, crop_size)\n",
    "    return image, mask\n",
    "\n",
    "#resize to original size\n",
    "def resize(image, mask, size):\n",
    "    image = cv2.resize(image, size)\n",
    "    mask = cv2.resize(mask, size)\n",
    "    return image, mask\n",
    "\n",
    "#apply augmentation and then resize to original size\n",
    "def augment_and_resize(image, mask, angle_range, crop_size, size):\n",
    "    image, mask = random_augmentation(image, mask, angle_range, crop_size)\n",
    "    image, mask = resize(image, mask, size)\n",
    "    return image, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc174d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#augment dataset and dump to disk in pt format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e81b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain yolo segmentation net on the new data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.dataset[idx]['img'])\n",
    "        maskfile = h5py.File(self.dataset[idx]['mask'], 'r')\n",
    "        \n",
    "        for group in maskfile:\n",
    "            for frame in maskfile[group]:\n",
    "                mask = np.array( maskfile[group][frame] ,  dtype = np.uint16 )\n",
    "                if np.sum(mask) > 0 :\n",
    "                    mask = mask\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "transform = transforms.Compose([ transforms.ToTensor() ])\n",
    "dataset = CustomDataset(dataset, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traininig loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        print(outputs.shape, targets.shape)\n",
    "        break\n",
    "\n",
    "\n",
    "#visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "for i in range(2):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.array(inputs[i].permute(1, 2, 0)))\n",
    "    plt.title('Input')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(targets[i].squeeze(), cmap='gray')\n",
    "    plt.title('Target')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(outputs[i].squeeze(), cmap='gray')\n",
    "    plt.title('Output')\n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
